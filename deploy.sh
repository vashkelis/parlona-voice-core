#!/bin/bash
#
# Parlona Core - Multi-Machine Deployment Script
# 
# This script helps deploy Parlona Core across multiple machines.
# Usage: ./deploy.sh [machine_number]
#
#   ./deploy.sh 1   - Setup Machine #1 (API Server + Workers)
#   ./deploy.sh 2   - Setup Machine #2 (STT GPU)
#   ./deploy.sh 3   - Setup Machine #3 (vLLM GPU)
#   ./deploy.sh     - Show this help
#

set -e

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
IMAGE_TAG="${IMAGE_TAG:-1.2.0}"
VLLM_MODEL="${VLLM_MODEL:-lmsys/vicuna-7b-v1.5}"

print_header() {
    echo -e "\n${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo -e "${BLUE}  $1${NC}"
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

print_step() {
    echo -e "${BLUE}ðŸ“‹ $1${NC}"
}

# Check if running as root or have docker access
check_docker() {
    if ! command -v docker &> /dev/null; then
        print_error "Docker is not installed. Please install Docker first."
        exit 1
    fi
    
    if ! docker info &> /dev/null; then
        print_error "Cannot connect to Docker. Please ensure Docker is running."
        exit 1
    fi
    
    print_success "Docker is available"
}

# Pull images
pull_images() {
    print_step "Pulling Docker images from Docker Hub..."
    
    echo "Pulling Parlona Core images..."
    docker pull "parlona/voicecore-api:${IMAGE_TAG}" || print_warning "Failed to pull api image"
    docker pull "parlona/voicecore-summary:${IMAGE_TAG}" || print_warning "Failed to pull summary image"
    docker pull "parlona/voicecore-postprocess:${IMAGE_TAG}" || print_warning "Failed to pull postprocess image"
    docker pull "parlona/voicecore-stt:${IMAGE_TAG}" || print_warning "Failed to pull stt image"
    
    echo "Pulling base images..."
    docker pull redis:7-alpine || print_warning "Failed to pull redis"
    docker pull postgres:16 || print_warning "Failed to pull postgres"
    docker pull vllm/vllm-openai:latest || print_warning "Failed to pull vllm"
    
    print_success "Images pulled"
}

# Setup Machine 1 - API Server + Workers
setup_machine_1() {
    print_header "Machine #1: API Server + Workers"
    
    echo "This machine will run: API, Summary, Postprocess services"
    echo ""
    
    # Get configuration
    read -p "REDIS Host IP (or press Enter for localhost): " REDIS_HOST
    REDIS_HOST="${REDIS_HOST:-localhost}"
    
    read -p "PostgreSQL Host IP (or press Enter for localhost): " POSTGRES_HOST
    POSTGRES_HOST="${POSTGRES_HOST:-localhost}"
    
    read -p "vLLM Host IP (optional, press Enter to skip): " VLLM_HOST
    
    # Create .env file
    print_step "Creating .env file..."
    
    cat > .env << EOF
# Parlona Core Environment Configuration
# Generated by deploy.sh

# === Required ===
CALL_API_KEY=$(openssl rand -hex 32)
REDIS_PASSWORD=$(openssl rand -hex 16)
POSTGRES_PASSWORD=$(openssl rand -hex 16)
POSTGRES_USER=parlonacore
POSTGRES_DB=parlonacore

# === Service Hosts ===
REDIS_HOST=${REDIS_HOST}
POSTGRES_HOST=${POSTGRES_HOST}

# === Optional: vLLM (fill if Machine #3 will be used) ===
EOF

    if [ -n "$VLLM_HOST" ]; then
        cat >> .env << EOF
LLM_BACKEND=vllm
VLLM_API_BASE=http://${VLLM_HOST}:8000/v1
VLLM_API_KEY=EMPTY
VLLM_MODEL=${VLLM_MODEL}
EOF
        print_warning "vLLM configured - make sure Machine #3 is running!"
    else
        cat >> .env << EOF
# Uncomment below and fill if using remote vLLM:
# LLM_BACKEND=vllm
# VLLM_API_BASE=http://vllm-host:8000/v1
# VLLM_API_KEY=EMPTY
# VLLM_MODEL=${VLLM_MODEL}
EOF
    fi
    
    print_success "Created .env file"
    print_warning "IMPORTANT: Save this .env file securely!"
    echo ""
    echo "Contents:"
    cat .env
    echo ""
    
    read -p "Continue? (y/n): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
    
    # Copy docker-compose.cpu.yml
    print_step "Setting up Docker Compose..."
    
    # Use the existing docker-compose.cpu.yml
    if [ ! -f "docker-compose.cpu.yml" ]; then
        print_error "docker-compose.cpu.yml not found!"
        exit 1
    fi
    
    # Start services
    print_step "Starting services..."
    docker compose -f docker-compose.cpu.yml up -d
    
    echo ""
    print_success "Machine #1 setup complete!"
    echo ""
    echo "Services running:"
    docker compose -f docker-compose.cpu.yml ps
    echo ""
    echo "API available at: http://localhost:8080"
    echo ""
    echo "Next steps:"
    echo "  1. If using STT on Machine #2: Note REDIS_HOST=${REDIS_HOST}"
    echo "  2. If using vLLM on Machine #3: Configure and restart summary_service"
}

# Setup Machine 2 - STT GPU
setup_machine_2() {
    print_header "Machine #2: STT GPU Worker"
    
    echo "This machine will run: STT Service (Whisper on GPU)"
    echo ""
    
    # Get Redis connection
    read -p "REDIS Host IP: " REDIS_HOST
    read -p "REDIS Password: " REDIS_PASSWORD
    
    # Create storage
    print_step "Creating storage directory..."
    mkdir -p storage
    
    # Create .env
    cat > .env << EOF
REDIS_HOST=${REDIS_HOST}
REDIS_PASSWORD=${REDIS_PASSWORD}
EOF
    
    # Run STT container
    print_step "Starting STT GPU container..."
    
    docker run -d \
        --name stt-gpu \
        --gpus all \
        --restart unless-stopped \
        -e REDIS_URL="redis://:${REDIS_PASSWORD}@${REDIS_HOST}:6379/0" \
        -e STT_ENABLE_GPU=1 \
        -e WHISPER_DEVICE=cuda \
        -e STT_DIARIZATION_MODE=stereo_channels \
        -e STT_STEREO_SPEAKER_MAPPING=0:agent,1:customer \
        -e WHISPER_LOCAL_ONLY=0 \
        -v "$(pwd)/storage:/app/storage" \
        "parlona/voicecore-stt:${IMAGE_TAG}"
    
    echo ""
    print_success "STT GPU worker started!"
    echo ""
    echo "Check status:"
    echo "  docker logs -f stt-gpu"
    echo ""
    echo "Stop:"
    echo "  docker stop stt-gpu"
}

# Setup Machine 3 - vLLM GPU
setup_machine_3() {
    print_header "Machine #3: vLLM GPU Server"
    
    echo "This machine will run: vLLM Server for LLM inference"
    echo ""
    
    # Get configuration
    read -p "Model name [${VLLM_MODEL}]: " INPUT_MODEL
    INPUT_MODEL="${INPUT_MODEL:-${VLLM_MODEL}}"
    
    read -p "API Key [EMPTY]: " API_KEY
    API_KEY="${API_KEY:-EMPTY}"
    
    read -p "Port [8000]: " PORT
    PORT="${PORT:-8000}"
    
    # Create directories
    print_step "Creating directories..."
    mkdir -p models storage
    
    # Run vLLM
    print_step "Starting vLLM container..."
    echo "Model: ${INPUT_MODEL}"
    echo "This may take a few minutes on first run to download the model..."
    
    docker run -d \
        --name vllm-server \
        --gpus all \
        -v "$(pwd)/models:/models" \
        -p "${PORT}:${PORT}" \
        -p $((PORT+1)):$((PORT+1)) \
        --restart unless-stopped \
        vllm/vllm-openai:latest \
        --model "${INPUT_MODEL}" \
        --api-key "${API_KEY}" \
        --host 0.0.0.0 \
        --port ${PORT} \
        --tensor-parallel-size 1
    
    echo ""
    print_success "vLLM server started!"
    echo ""
    echo "Model: ${INPUT_MODEL}"
    echo "API URL: http://localhost:${PORT}/v1"
    echo ""
    echo "To use from Machine #1, update .env:"
    echo "  LLM_BACKEND=vllm"
    echo "  VLLM_API_BASE=http://$(hostname -I | awk '{print $1}'):${PORT}/v1"
    echo "  VLLM_API_KEY=${API_KEY}"
    echo "  VLLM_MODEL=${INPUT_MODEL}"
    echo ""
    echo "Then restart summary service:"
    echo "  docker compose -f docker-compose.cpu.yml restart summary_service"
}

# Show help
show_help() {
    echo -e "${BLUE}Parlona Core - Multi-Machine Deployment${NC}\n"
    echo "Usage: $0 [machine_number]\n"
    echo "Machine numbers:"
    echo "  1   - Setup Machine #1 (API Server + Workers)"
    echo "  2   - Setup Machine #2 (STT GPU)"
    echo "  3   - Setup Machine #3 (vLLM GPU)"
    echo ""
    echo "Environment variables:"
    echo "  IMAGE_TAG    - Docker image tag (default: 1.2.0)"
    echo "  VLLM_MODEL   - vLLM model name (default: lmsys/vicuna-7b-v1.5)"
    echo ""
    echo "Examples:"
    echo "  $0 1                    # Setup API server"
    echo "  $0 2                    # Setup STT GPU worker"
    echo "  IMAGE_TAG=1.0.0 $0 3   # Setup vLLM with custom version"
}

# Main
case "${1:-}" in
    1)
        check_docker
        pull_images
        setup_machine_1
        ;;
    2)
        check_docker
        pull_images
        setup_machine_2
        ;;
    3)
        check_docker
        pull_images
        setup_machine_3
        ;;
    help|--help|-h|"")
        show_help
        ;;
    *)
        print_error "Unknown option: $1"
        show_help
        exit 1
        ;;
esac
